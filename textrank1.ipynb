{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "class TextRank4Keyword():\n",
    "    \"\"\"A compiler is a special program that processes statements written in a particular programming language and turns them into machine language There are two parts to compile: analysis and synthesis. Symbol Table is an important data structure created and maintained by the compiler in order to keep track of the semantics of variable basically have two phases of compilers, namely Analysis phase and Synthesis phase. The LR parser is a non recursive, shift reduce, bottom up parser. Its use as a wide class of context-free grammar\n",
    "\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "            \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "    # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            print(key + ' - ' + str(value))\n",
    "            if i > number:\n",
    "                break\n",
    "        \n",
    "        \n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'PROPN'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "    # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (3.0.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy) (2.11.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy) (50.3.1.post20201107)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy) (2.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy) (0.5.2)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy) (4.50.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy) (1.19.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy) (20.4)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy) (1.7.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy) (8.0.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (3.0.0)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (1.15.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nlp\n",
      "  Downloading nlp-0.4.0-py3-none-any.whl (1.7 MB)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
      "Collecting pyarrow>=0.16.0\n",
      "  Downloading pyarrow-4.0.0-cp38-cp38-win_amd64.whl (13.3 MB)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from nlp) (2.24.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from nlp) (1.1.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from nlp) (1.19.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from nlp) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from nlp) (4.50.2)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp38-cp38-win_amd64.whl (35 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from requests>=2.19.0->nlp) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from requests>=2.19.0->nlp) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from requests>=2.19.0->nlp) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from requests>=2.19.0->nlp) (1.25.11)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from pandas->nlp) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from pandas->nlp) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->nlp) (1.15.0)\n",
      "Installing collected packages: dill, pyarrow, xxhash, nlp\n",
      "Successfully installed dill-0.3.3 nlp-0.4.0 pyarrow-4.0.0 xxhash-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7 MB)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (50.3.1.post20201107)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.50.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.24.0)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.5.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.15.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.25.11)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in c:\\users\\keerthi\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.0.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " A compiler is a special program that processes statements written in a particular programming language and turns them into machine language\n",
      "There are two parts to compile: analysis and synthesis.\n",
      "Symbol Table is an important data structure created and maintained by the compiler in order to keep track of the semantics of variable basically have two phases of compilers, namely Analysis phase and Synthesis phase.\n",
      "The LR parser is a non recursive, shift reduce, bottom up parser.\n",
      "Its use as a wide class of context-free grammar\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "content = '''\n",
    " A compiler is a special program that processes statements written in a particular programming language and turns them into machine language There are two parts to compile: analysis and synthesis. Symbol Table is an important data structure created and maintained by the compiler in order to keep track of the semantics of variable basically have two phases of compilers, namely Analysis phase and Synthesis phase. The LR parser is a non recursive, shift reduce, bottom up parser. Its use as a wide class of context-free grammar\n",
    "'''\n",
    "doc = nlp(content)\n",
    "for sents in doc.sents:\n",
    "    print(sents.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiler - 1.568311574074074\n",
      "parser - 1.344722222222222\n",
      "language - 1.1463416666666666\n",
      "structure - 1.1233680555555556\n",
      "phase - 1.0917291666666666\n",
      "order - 1.064733796296296\n",
      "compilers - 1.0583587962962961\n",
      "phases - 1.0450578703703703\n",
      "track - 1.0404143518518518\n",
      "semantics - 1.027034722222222\n",
      "variable - 1.0182986111111112\n",
      "parts - 1.0\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    " A compiler is a special program that processes statements written in a particular programming language and turns them into machine language There are two parts to compile: analysis and synthesis. Symbol Table is an important data structure created and maintained by the compiler in order to keep track of the semantics of variable basically have two phases of compilers, namely Analysis phase and Synthesis phase. The LR parser is a non recursive, shift reduce, bottom up parser. Its use as a wide class of context-free grammar\n",
    "'''\n",
    "\n",
    "tr4w = TextRank4Keyword()\n",
    "tr4w.analyze(text, candidate_pos = ['NOUN', 'PROPN'], window_size=4, lower=False)\n",
    "tr4w.get_keywords(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
